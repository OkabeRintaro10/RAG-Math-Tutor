{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a19bbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ryefoxlime/MathTutor/notebook\n",
      "/home/ryefoxlime/MathTutor\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir(\"../\")\n",
    "print(os.getcwd())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f7ef489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "import os\n",
    "import uuid\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from fastapi import HTTPException\n",
    "from guardrails.errors import ValidationError\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import END, StateGraph\n",
    "\n",
    "import inngest\n",
    "from guard.guardrails import InputGuard, OutputGuard\n",
    "from src.Math import logger\n",
    "from src.Math.components.data_ingestion import DataLoader\n",
    "from src.Math.components.data_storing import QdrantStorage\n",
    "from src.Math.config.configuration import ConfigurationManager\n",
    "from src.Math.entity.config_entity import (\n",
    "    GraphState,\n",
    ")\n",
    "\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "787be2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-11-09 16:58:51,891: INFO: common]: yaml file: config/config.yaml loaded successfully]\n",
      "[2025-11-09 16:58:51,893: INFO: common]: yaml file: params.yaml loaded successfully]\n"
     ]
    }
   ],
   "source": [
    "FEEDBACK_FILE = \"feedback.jsonl\"\n",
    "\n",
    "config_manager = ConfigurationManager()\n",
    "data_ingestion_config = config_manager.get_data_ingestion_config()\n",
    "qdrant_config = config_manager.get_data_storing_params()\n",
    "\n",
    "data_loader = DataLoader(data_ingestion_config)\n",
    "qdrant_storage = QdrantStorage(config=qdrant_config)\n",
    "\n",
    "input_guard = InputGuard()\n",
    "output_guard = OutputGuard()\n",
    "\n",
    "model_name = config_manager.config.models[0].parameters.model\n",
    "base_url = config_manager.config.models[0].parameters.base_url\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "\n",
    "# Initialize Inngest client\n",
    "inngest_client = inngest.Inngest(\n",
    "    app_id=\"rag_app\",\n",
    "    logger=logger,\n",
    "    is_production=False,\n",
    "    serializer=inngest.PydanticSerializer(),\n",
    ")\n",
    "\n",
    "SUMMARY_THRESHOLD = 102400\n",
    "\n",
    "RELEVANCE_THRESHOLD = 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c724c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_context(state: GraphState) -> dict[str]:\n",
    "            \"\"\"Prepare context from conversation history.\"\"\"\n",
    "            logger.debug(\"Running prepare_context node\")\n",
    "            history = state.history\n",
    "            question = state.question\n",
    "            current_history_tokens = state.history_tokens\n",
    "\n",
    "            if not history:\n",
    "                return {\"summary\": \"\", \"history_tokens\": 0}\n",
    "\n",
    "            logger.info(f\"Context threshold set to {SUMMARY_THRESHOLD} tokens.\")\n",
    "\n",
    "            if current_history_tokens <= SUMMARY_THRESHOLD:\n",
    "                logger.info(\"History is below threshold. Using full history.\")\n",
    "                prompt_history = []\n",
    "                for msg in history:\n",
    "                    role = \"User\" if msg[\"sender\"] == \"user\" else \"Assistant\"\n",
    "                    prompt_history.append(f\"{role}: {msg['text']}\")\n",
    "                history_str = \"\\n\".join(prompt_history)\n",
    "\n",
    "                return {\n",
    "                    \"summary\": history_str,\n",
    "                    \"history_tokens\": current_history_tokens,\n",
    "                }\n",
    "            else:\n",
    "                logger.info(\"History is over threshold. Summarizing...\")\n",
    "                prompt_history = []\n",
    "                for msg in history:\n",
    "                    role = \"User\" if msg[\"sender\"] == \"user\" else \"Assistant\"\n",
    "                    prompt_history.append(f\"{role}: {msg['text']}\")\n",
    "                history_str = \"\\n\".join(prompt_history)\n",
    "\n",
    "                summary_prompt = (\n",
    "                    \"You are a helpful summarization assistant. \"\n",
    "                    \"Condense the following conversation into a short paragraph. \"\n",
    "                    \"Maintain Key Topics from the conversation especially any \"\n",
    "                    \"formulas or transformations. \"\n",
    "                    \"Focus on the main topics and any information relevant to the \"\n",
    "                    \"user's *new* question.\\n\\n\"\n",
    "                    \"--- CONVERSATION ---\\n\"\n",
    "                    f\"{history_str}\\n\\n\"\n",
    "                    \"--- USER'S NEW QUESTION ---\\n\"\n",
    "                    f\"{question}\\n\\n\"\n",
    "                    \"--- CONCISE SUMMARY ---\\n\"\n",
    "                )\n",
    "                try:\n",
    "                    summarizer_llm = ChatOpenAI(\n",
    "                        model=model_name,\n",
    "                        base_url=base_url,\n",
    "                        api_key=api_key,\n",
    "                    )\n",
    "                    # No async needed here, it's a small internal call\n",
    "                    response = summarizer_llm.invoke(summary_prompt)\n",
    "\n",
    "                    new_summary = getattr(response, \"content\", str(response))\n",
    "                    new_token_count = response.response_metadata[\"token_usage\"][\n",
    "                        \"total_tokens\"\n",
    "                    ]\n",
    "\n",
    "                    logger.info(\n",
    "                        f\"Summarization complete. New token count: {new_token_count}\"\n",
    "                    )\n",
    "                    return {\"summary\": new_summary, \"history_tokens\": new_token_count}\n",
    "\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Summarization failed: {e}\")\n",
    "                    return {\"summary\": \"\", \"history_tokens\": 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "055da735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_question(state: GraphState) -> dict[str]:\n",
    "    \"\"\"Validate that the question is math-related.\"\"\"\n",
    "    logger.info(\"Running validate_question node\")\n",
    "    question = state.question\n",
    "    try:\n",
    "        input_guard.validate(text_to_validate=question)\n",
    "        logger.info(\"Input validation passed.\")\n",
    "        return {\"is_valid\": True}\n",
    "    except ValidationError as e:\n",
    "        logger.error(\"Input validation failed: %s\", e)\n",
    "        return {\n",
    "            \"is_valid\": False,\n",
    "            \"generation\": \"I can only answer math-related questions.\",\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3baaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: GraphState) -> dict[str]:\n",
    "            \"\"\"Retrieve relevant documents from vector store.\"\"\"\n",
    "            question = state.question\n",
    "            logger.info(\"Retrieving documents for question: %s\", question[:50])\n",
    "\n",
    "            query_vec = data_loader.embed_query(question)\n",
    "\n",
    "            # --- MODIFICATION ---\n",
    "            # Pass the RELEVANCE_THRESHOLD to the search method.\n",
    "            # data_storing.py will now try to use this for DB-level filtering.\n",
    "            found = qdrant_storage.search(\n",
    "                query_vec, 5, score_threshold=RELEVANCE_THRESHOLD\n",
    "            )\n",
    "\n",
    "            if not found or not found.get(\"contexts\"):\n",
    "                logger.warning(\"No search results returned from Qdrant\")\n",
    "                return {\"documents\": [], \"is_kb_relevant\": False}\n",
    "\n",
    "            documents = found.get(\"contexts\", [])\n",
    "            scores = found.get(\"scores\", [])  # Get scores\n",
    "\n",
    "            if not scores:\n",
    "                # Fallback if search method returns no scores\n",
    "                logger.warning(\"No scores returned from Qdrant. Using simple check.\")\n",
    "                is_kb_relevant = bool(\n",
    "                    documents and any(doc.strip() for doc in documents)\n",
    "                )\n",
    "            else:\n",
    "                top_score = scores[0] if scores else 0\n",
    "                logger.info(f\"Top document score: {top_score}\")\n",
    "\n",
    "                # --- IMPORTANT ---\n",
    "                # We KEEP this application-level filter.\n",
    "                # Why? In case the 'score_threshold' argument failed silently\n",
    "                # (due to the old client), this ensures we still filter out\n",
    "                # irrelevant results.\n",
    "                if top_score >= RELEVANCE_THRESHOLD:\n",
    "                    is_kb_relevant = True\n",
    "                    # Filter documents to only include those above the threshold\n",
    "                    documents = [\n",
    "                        doc\n",
    "                        for doc, score in zip(documents, scores)\n",
    "                        if score >= RELEVANCE_THRESHOLD\n",
    "                    ]\n",
    "                    logger.info(\n",
    "                        f\"Found {len(documents)} relevant docs above threshold.\"\n",
    "                    )\n",
    "                else:\n",
    "                    is_kb_relevant = False\n",
    "                    documents = []  # Discard irrelevant documents\n",
    "                    logger.info(\"Top score below threshold. Ignoring KB.\")\n",
    "\n",
    "            logger.info(\"KB relevance: %s\", is_kb_relevant)\n",
    "\n",
    "            return {\n",
    "                \"documents\": documents,\n",
    "                \"is_kb_relevant\": is_kb_relevant,\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da7922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_web_search(state: GraphState) -> str:\n",
    "    \"\"\"Decide whether to use web search or generate from KB.\"\"\"\n",
    "    is_kb_relevant = state.is_kb_relevant\n",
    "    logger.debug(\"should_web_search check: is_kb_relevant=%s\", is_kb_relevant)\n",
    "\n",
    "    if is_kb_relevant:\n",
    "        return \"prepare_context\"\n",
    "    return \"web_search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c0c7fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state: GraphState) -> dict[str]:\n",
    "            \"\"\"Generate answer using LLM with output guardrails.\"\"\"\n",
    "            logger.debug(\"Running generate node\")\n",
    "            question = state.question\n",
    "            documents = state.documents\n",
    "            summary = state.summary\n",
    "            current_history_tokens = state.history_tokens\n",
    "\n",
    "            valid_docs = [doc for doc in documents if doc and doc.strip()]\n",
    "            context_str = \"\\n\\n\".join(valid_docs) if valid_docs else \"\"\n",
    "\n",
    "            if context_str:\n",
    "                prompt = (\n",
    "                    \"You are a helpful math assistant. Use the following context to \"\n",
    "                    \"answer the question accurately.\\n\"\n",
    "                    \"If the context doesn't contain enough information, say \"\n",
    "                    '\"I don\\'t have enough information to answer this question.\"\\n\\n'\n",
    "                    f\"Context:\\n{context_str}\\n\\n\"\n",
    "                    f\"Conversation History:\\n{summary}\\n\\n\"\n",
    "                    f\"Question:\\n{question}\\n\\n\"\n",
    "                    \"Answer:\\n\"\n",
    "                )\n",
    "            else:\n",
    "                prompt = (\n",
    "                    \"You are a helpful math assistant. Answer the following question.\\n\"\n",
    "                    \"If you don't know the answer, say \"\n",
    "                    '\"I don\\'t have enough information to answer this question.\"\\n\\n'\n",
    "                    f\"Conversation History:\\n{summary}\\n\\n\"\n",
    "                    f\"Question:\\n{question}\\n\\n\"\n",
    "                    \"Answer:\\n\"\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                llm = ChatOpenAI(\n",
    "                    model=model_name, base_url=base_url, api_key=api_key\n",
    "                )\n",
    "                # Use ainovke for async LLM call\n",
    "                response = llm.ainvoke(prompt)\n",
    "                generation_cost = response.response_metadata[\"token_usage\"][\n",
    "                    \"total_tokens\"\n",
    "                ]\n",
    "                new_total_tokens = current_history_tokens + generation_cost\n",
    "                content = getattr(response, \"content\", str(response))\n",
    "\n",
    "                output_guard.validate(text_to_validate=content)\n",
    "                logger.info(\n",
    "                    \"LLM generation complete and validated (len=%d)\", len(content)\n",
    "                )\n",
    "                return {\"generation\": content, \"history_tokens\": new_total_tokens}\n",
    "\n",
    "            except ValidationError as ve:\n",
    "                logger.warning(\"Output Guardrail Failed: %s\", ve)\n",
    "                return {\n",
    "                    \"generation\": str(ve),\n",
    "                    \"history_tokens\": current_history_tokens,\n",
    "                }\n",
    "            except Exception as exc:\n",
    "                logger.exception(\"LLM generation failed: %s\", exc)\n",
    "                return {\n",
    "                    \"generation\": (\n",
    "                        \"An error occurred while generating the answer. \"\n",
    "                        \"Please try again.\"\n",
    "                    ),\n",
    "                    \"history_tokens\": current_history_tokens,\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46068476",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"prepare_context\", prepare_context)\n",
    "workflow.add_node(\"validate_question\", validate_question)\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"should_web_search\", should_web_search)  # This is now async\n",
    "workflow.add_node(\"generate\", generate)  # This is now async\n",
    "\n",
    "workflow.set_entry_point(\"validate_question\")\n",
    "\n",
    "def after_validation(state: GraphState) -> str:\n",
    "    return \"retrieve\" if state.is_valid else END\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"validate_question\",\n",
    "    after_validation,\n",
    "    {END: END, \"retrieve\": \"retrieve\"},\n",
    ")\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    should_web_search,\n",
    "    {\"should_web_search\": \"should_web_search\", \"prepare_context\": \"prepare_context\"},\n",
    ")\n",
    "workflow.add_edge(\"should_web_search\", END)\n",
    "workflow.add_edge(\"prepare_context\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "app_graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3346d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(question: str) -> dict[str, Any]:\n",
    "    \"\"\"Query the RAG pipeline directly without Inngest.\"\"\"\n",
    "    if not question:\n",
    "        raise ValueError(\"Question cannot be empty\")\n",
    "\n",
    "    inputs = {\n",
    "        \"question\": question,\n",
    "        \"is_kb_relevant\": False,\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # This should now work correctly with the async nodes\n",
    "        final_state = app_graph.ainvoke(inputs)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Graph ainvoke failed: {e}\", exc_info=True)\n",
    "        return {\n",
    "            \"answer\": f\"An error occurred during pipeline execution: {e}\",\n",
    "            \"sources\": [],\n",
    "        }\n",
    "\n",
    "    generation = final_state.get(\"generation\")\n",
    "\n",
    "    return {\"answer\": generation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MathTutor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
